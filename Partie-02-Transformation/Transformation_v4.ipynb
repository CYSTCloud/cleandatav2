{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Partie 2 : Transformation des Données (Version 4)\n",
        "\n",
        "Ce notebook présente le processus de transformation des données brutes pour notre projet ETL sur les pandémies. Nous allons nettoyer, agréger, normaliser et supprimer les doublons des données extraites dans la phase précédente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "\n",
        "# Ignorer les avertissements\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration pour afficher plus de colonnes\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Vérification des chemins et création du répertoire de sortie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chemins des fichiers de données\n",
        "covid_data_path = '../Partie-01-Extraction/Donnees Brutes/covid_19_clean_complete.csv'\n",
        "monkeypox_data_path = '../Partie-01-Extraction/Donnees Brutes/owid-monkeypox-data.csv'\n",
        "worldometer_data_path = '../Partie-01-Extraction/Donnees Brutes/worldometer_coronavirus_daily_data.csv'\n",
        "\n",
        "# Vérification de l'existence des fichiers\n",
        "files_exist = True\n",
        "for file_path in [covid_data_path, monkeypox_data_path, worldometer_data_path]:\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"ERREUR: Le fichier {file_path} n'existe pas.\")\n",
        "        files_exist = False\n",
        "    else:\n",
        "        print(f\"Le fichier {file_path} existe.\")\n",
        "\n",
        "if not files_exist:\n",
        "    print(\"Certains fichiers de données sont manquants. Veuillez vérifier les chemins.\")\n",
        "\n",
        "# Création du répertoire de sortie\n",
        "output_dir = '../Partie-03-Chargement/donnees_nettoyees/'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "print(f\"Répertoire de sortie créé: {output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Chargement des données brutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fonction pour charger un fichier CSV avec gestion des erreurs\n",
        "def load_csv_file(file_path, file_desc):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        print(f\"Données {file_desc} chargées avec succès. Forme: {df.shape}\")\n",
        "        display(df.head())\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur lors du chargement des données {file_desc}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Chargement des données COVID-19\n",
        "covid_df = load_csv_file(covid_data_path, \"COVID-19\")\n",
        "\n",
        "# Chargement des données Monkeypox\n",
        "monkeypox_df = load_csv_file(monkeypox_data_path, \"Monkeypox\")\n",
        "\n",
        "# Chargement des données Worldometer\n",
        "worldometer_df = load_csv_file(worldometer_data_path, \"Worldometer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Nettoyage des données COVID-19"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_covid_data(df):\n",
        "    if df is None:\n",
        "        return None\n",
        "    \n",
        "    # Copie du DataFrame\n",
        "    cleaned_df = df.copy()\n",
        "    \n",
        "    # Conversion des dates\n",
        "    cleaned_df['Date'] = pd.to_datetime(cleaned_df['Date'])\n",
        "    \n",
        "    # Normalisation des noms de colonnes\n",
        "    cleaned_df.columns = [col.lower().replace(' ', '_') for col in cleaned_df.columns]\n",
        "    \n",
        "    # Remplacement des valeurs manquantes par 0 pour les colonnes numériques\n",
        "    for col in cleaned_df.select_dtypes(include=['float64', 'int64']).columns:\n",
        "        cleaned_df[col] = cleaned_df[col].fillna(0)\n",
        "    \n",
        "    # Suppression des doublons\n",
        "    cleaned_df = cleaned_df.drop_duplicates()\n",
        "    \n",
        "    return cleaned_df\n",
        "\n",
        "# Nettoyage des données COVID-19\n",
        "covid_cleaned = clean_covid_data(covid_df)\n",
        "if covid_cleaned is not None:\n",
        "    print(f\"Données COVID-19 nettoyées. Forme: {covid_cleaned.shape}\")\n",
        "    display(covid_cleaned.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Nettoyage des données Monkeypox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_monkeypox_data(df):\n",
        "    if df is None:\n",
        "        return None\n",
        "    \n",
        "    # Copie du DataFrame\n",
        "    cleaned_df = df.copy()\n",
        "    \n",
        "    # Conversion des dates\n",
        "    if 'date' in cleaned_df.columns:\n",
        "        cleaned_df['date'] = pd.to_datetime(cleaned_df['date'])\n",
        "    \n",
        "    # Normalisation des noms de colonnes\n",
        "    cleaned_df.columns = [col.lower().replace(' ', '_') for col in cleaned_df.columns]\n",
        "    \n",
        "    # Remplacement des valeurs manquantes par 0 pour les colonnes numériques\n",
        "    for col in cleaned_df.select_dtypes(include=['float64', 'int64']).columns:\n",
        "        cleaned_df[col] = cleaned_df[col].fillna(0)\n",
        "    \n",
        "    # Suppression des doublons\n",
        "    cleaned_df = cleaned_df.drop_duplicates()\n",
        "    \n",
        "    return cleaned_df\n",
        "\n",
        "# Nettoyage des données Monkeypox\n",
        "monkeypox_cleaned = clean_monkeypox_data(monkeypox_df)\n",
        "if monkeypox_cleaned is not None:\n",
        "    print(f\"Données Monkeypox nettoyées. Forme: {monkeypox_cleaned.shape}\")\n",
        "    display(monkeypox_cleaned.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Nettoyage des données Worldometer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_worldometer_data(df):\n",
        "    if df is None:\n",
        "        return None\n",
        "    \n",
        "    # Copie du DataFrame\n",
        "    cleaned_df = df.copy()\n",
        "    \n",
        "    # Conversion des dates\n",
        "    if 'date' in cleaned_df.columns:\n",
        "        cleaned_df['date'] = pd.to_datetime(cleaned_df['date'])\n",
        "    \n",
        "    # Normalisation des noms de colonnes\n",
        "    cleaned_df.columns = [col.lower().replace(' ', '_') for col in cleaned_df.columns]\n",
        "    \n",
        "    # Remplacement des valeurs manquantes par 0 pour les colonnes numériques\n",
        "    for col in cleaned_df.select_dtypes(include=['float64', 'int64']).columns:\n",
        "        cleaned_df[col] = cleaned_df[col].fillna(0)\n",
        "    \n",
        "    # Suppression des doublons\n",
        "    cleaned_df = cleaned_df.drop_duplicates()\n",
        "    \n",
        "    return cleaned_df\n",
        "\n",
        "# Nettoyage des données Worldometer\n",
        "worldometer_cleaned = clean_worldometer_data(worldometer_df)\n",
        "if worldometer_cleaned is not None:\n",
        "    print(f\"Données Worldometer nettoyées. Forme: {worldometer_cleaned.shape}\")\n",
        "    display(worldometer_cleaned.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Préparation des tables pour le chargement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Préparation de la table des localisations\n",
        "def prepare_locations():\n",
        "    # Création d'une liste pour stocker les localisations\n",
        "    locations = []\n",
        "    \n",
        "    # Extraction des localisations depuis COVID-19\n",
        "    if covid_cleaned is not None and 'country_region' in covid_cleaned.columns:\n",
        "        covid_locs = covid_cleaned[['country_region']].drop_duplicates()\n",
        "        covid_locs = covid_locs.rename(columns={'country_region': 'pays'})\n",
        "        locations.append(covid_locs)\n",
        "    \n",
        "    # Extraction des localisations depuis Monkeypox\n",
        "    if monkeypox_cleaned is not None and 'entity' in monkeypox_cleaned.columns:\n",
        "        mpox_locs = monkeypox_cleaned[['entity']].drop_duplicates()\n",
        "        mpox_locs = mpox_locs.rename(columns={'entity': 'pays'})\n",
        "        locations.append(mpox_locs)\n",
        "    \n",
        "    # Fusion et suppression des doublons\n",
        "    if locations:\n",
        "        locations_df = pd.concat(locations, ignore_index=True)\n",
        "        locations_df = locations_df.drop_duplicates()\n",
        "        \n",
        "        # Ajout des colonnes nécessaires selon le schéma SQL\n",
        "        locations_df['id_localisation'] = range(1, len(locations_df) + 1)\n",
        "        locations_df['code_pays'] = None\n",
        "        locations_df['region'] = None\n",
        "        locations_df['continent'] = None\n",
        "        locations_df['latitude'] = None\n",
        "        locations_df['longitude'] = None\n",
        "        locations_df['population'] = None\n",
        "        \n",
        "        # Réorganisation des colonnes selon le schéma SQL\n",
        "        locations_df = locations_df[['id_localisation', 'pays', 'code_pays', 'region', 'continent', 'latitude', 'longitude', 'population']]\n",
        "        \n",
        "        return locations_df\n",
        "    else:\n",
        "        return pd.DataFrame(columns=['id_localisation', 'pays', 'code_pays', 'region', 'continent', 'latitude', 'longitude', 'population'])\n",
        "\n",
        "# Préparation de la table des pandémies\n",
        "def prepare_pandemics():\n",
        "    # Création manuelle de la table des pandémies selon le schéma SQL\n",
        "    pandemics_df = pd.DataFrame({\n",
        "        'id_pandemie': [1, 2],\n",
        "        'nom': ['COVID-19', 'Monkeypox'],\n",
        "        'agent_pathogene': ['SARS-CoV-2', 'Monkeypox virus'],\n",
        "        'description': [\n",
        "            'Maladie respiratoire causée par le coronavirus SARS-CoV-2 identifiée pour la première fois à Wuhan Chine en décembre 2019',\n",
        "            'Maladie virale zoonotique causée par le virus de la variole du singe appartenant au genre Orthopoxvirus'\n",
        "        ],\n",
        "        'date_debut': ['2019-12-01', '2022-05-01'],\n",
        "        'date_fin': [None, None]\n",
        "    })\n",
        "    \n",
        "    return pandemics_df\n",
        "\n",
        "# Création des tables\n",
        "locations_df = prepare_locations()\n",
        "pandemics_df = prepare_pandemics()\n",
        "\n",
        "print(f\"Table des localisations créée avec {len(locations_df)} entrées.\")\n",
        "display(locations_df.head())\n",
        "\n",
        "print(f\"Table des pandémies créée avec {len(pandemics_df)} entrées.\")\n",
        "display(pandemics_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Préparation des données pour le chargement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Préparation des données COVID-19 pour le chargement\n",
        "def prepare_covid_data_for_loading():\n",
        "    if covid_cleaned is None:\n",
        "        return None\n",
        "    \n",
        "    # Sélection des colonnes pertinentes\n",
        "    if all(col in covid_cleaned.columns for col in ['date', 'country_region', 'confirmed', 'deaths', 'recovered', 'active']):\n",
        "        covid_data = covid_cleaned[['date', 'country_region', 'confirmed', 'deaths', 'recovered', 'active']].copy()\n",
        "        \n",
        "        # Renommage des colonnes\n",
        "        covid_data = covid_data.rename(columns={\n",
        "            'date': 'date',\n",
        "            'country_region': 'pays',\n",
        "            'confirmed': 'cas_confirmes',\n",
        "            'deaths': 'deces',\n",
        "            'recovered': 'guerisons',\n",
        "            'active': 'cas_actifs'\n",
        "        })\n",
        "        \n",
        "        # Ajout de la colonne pandémie\n",
        "        covid_data['id_pandemie'] = 1\n",
        "        \n",
        "        return covid_data\n",
        "    else:\n",
        "        print(\"Colonnes requises manquantes dans les données COVID-19.\")\n",
        "        return None\n",
        "\n",
        "# Préparation des données Monkeypox pour le chargement\n",
        "def prepare_monkeypox_data_for_loading():\n",
        "    if monkeypox_cleaned is None:\n",
        "        return None\n",
        "    \n",
        "    # Sélection des colonnes pertinentes\n",
        "    if all(col in monkeypox_cleaned.columns for col in ['date', 'entity', 'confirmed_cases']):\n",
        "        mpox_data = monkeypox_cleaned[['date', 'entity', 'confirmed_cases']].copy()\n",
        "        \n",
        "        # Renommage des colonnes\n",
        "        mpox_data = mpox_data.rename(columns={\n",
        "            'date': 'date',\n",
        "            'entity': 'pays',\n",
        "            'confirmed_cases': 'cas_confirmes'\n",
        "        })\n",
        "        \n",
        "        # Ajout des colonnes manquantes\n",
        "        mpox_data['deces'] = 0\n",
        "        mpox_data['guerisons'] = 0\n",
        "        mpox_data['cas_actifs'] = mpox_data['cas_confirmes']\n",
        "        \n",
        "        # Ajout de la colonne pandémie\n",
        "        mpox_data['id_pandemie'] = 2\n",
        "        \n",
        "        return mpox_data\n",
        "    else:\n",
        "        print(\"Colonnes requises manquantes dans les données Monkeypox.\")\n",
        "        return None\n",
        "\n",
        "# Préparation des données finales\n",
        "covid_final = prepare_covid_data_for_loading()\n",
        "mpox_final = prepare_monkeypox_data_for_loading()\n",
        "\n",
        "# Affichage des données préparées\n",
        "if covid_final is not None:\n",
        "    print(f\"Données COVID-19 préparées: {covid_final.shape}\")\n",
        "    display(covid_final.head())\n",
        "    \n",
        "if mpox_final is not None:\n",
        "    print(f\"Données Monkeypox préparées: {mpox_final.shape}\")\n",
        "    display(mpox_final.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Enregistrement des données transformées"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fonction pour enregistrer les données transformées\n",
        "def save_data(df, filename):\n",
        "    if df is not None:\n",
        "        filepath = os.path.join(output_dir, filename)\n",
        "        df.to_csv(filepath, index=False)\n",
        "        print(f\"Données enregistrées dans {filepath}\")\n",
        "        return True\n",
        "    else:\n",
        "        print(f\"Impossible d'enregistrer {filename} car les données sont None.\")\n",
        "        return False\n",
        "\n",
        "# Enregistrement des tables de dimension\n",
        "save_data(locations_df, 'localisations_clean.csv')\n",
        "save_data(pandemics_df, 'pandemies_clean.csv')\n",
        "\n",
        "# Enregistrement des données spécifiques pour la phase de chargement\n",
        "if covid_final is not None:\n",
        "    # Cas confirmés COVID-19\n",
        "    cas_confirmes_covid = covid_final[['date', 'pays', 'cas_confirmes', 'id_pandemie']].copy()\n",
        "    save_data(cas_confirmes_covid, 'cas_confirmes_clean.csv')\n",
        "    \n",
        "    # Décès COVID-19\n",
        "    deces_covid = covid_final[['date', 'pays', 'deces', 'id_pandemie']].copy()\n",
        "    save_data(deces_covid, 'deces_clean.csv')\n",
        "    \n",
        "    # Guérisons COVID-19\n",
        "    guerisons_covid = covid_final[['date', 'pays', 'guerisons', 'id_pandemie']].copy()\n",
        "    save_data(guerisons_covid, 'guerisons_clean.csv')\n",
        "\n",
        "# Ajout des données Monkeypox aux fichiers existants\n",
        "if mpox_final is not None:\n",
        "    # Cas confirmés Monkeypox\n",
        "    cas_confirmes_mpox = mpox_final[['date', 'pays', 'cas_confirmes', 'id_pandemie']].copy()\n",
        "    cas_confirmes_file = os.path.join(output_dir, 'cas_confirmes_clean.csv')\n",
        "    \n",
        "    if os.path.exists(cas_confirmes_file):\n",
        "        # Lire le fichier existant\n",
        "        cas_confirmes_df = pd.read_csv(cas_confirmes_file)\n",
        "        # Concaténer avec les nouvelles données\n",
        "        cas_confirmes_df = pd.concat([cas_confirmes_df, cas_confirmes_mpox], ignore_index=True)\n",
        "        # Enregistrer le fichier mis à jour\n",
        "        cas_confirmes_df.to_csv(cas_confirmes_file, index=False)\n",
        "        print(f\"Données Monkeypox ajoutées à {cas_confirmes_file}\")\n",
        "    else:\n",
        "        save_data(cas_confirmes_mpox, 'cas_confirmes_clean.csv')\n",
        "    \n",
        "    # Décès Monkeypox\n",
        "    deces_mpox = mpox_final[['date', 'pays', 'deces', 'id_pandemie']].copy()\n",
        "    deces_file = os.path.join(output_dir, 'deces_clean.csv')\n",
        "    \n",
        "    if os.path.exists(deces_file):\n",
        "        deces_df = pd.read_csv(deces_file)\n",
        "        deces_df = pd.concat([deces_df, deces_mpox], ignore_index=True)\n",
        "        deces_df.to_csv(deces_file, index=False)\n",
        "        print(f\"Données Monkeypox ajoutées à {deces_file}\")\n",
        "    else:\n",
        "        save_data(deces_mpox, 'deces_clean.csv')\n",
        "    \n",
        "    # Guérisons Monkeypox\n",
        "    guerisons_mpox = mpox_final[['date', 'pays', 'guerisons', 'id_pandemie']].copy()\n",
        "    guerisons_file = os.path.join(output_dir, 'guerisons_clean.csv')\n",
        "    \n",
        "    if os.path.exists(guerisons_file):\n",
        "        guerisons_df = pd.read_csv(guerisons_file)\n",
        "        guerisons_df = pd.concat([guerisons_df, guerisons_mpox], ignore_index=True)\n",
        "        guerisons_df.to_csv(guerisons_file, index=False)\n",
        "        print(f\"Données Monkeypox ajoutées à {guerisons_file}\")\n",
        "    else:\n",
        "        save_data(guerisons_mpox, 'guerisons_clean.csv')\n",
        "\n",
        "print(\"\\nToutes les données ont été enregistrées dans le répertoire de sortie.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Vérification des fichiers générés"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vérification des fichiers générés\n",
        "output_files = os.listdir(output_dir)\n",
        "print(f\"Fichiers générés dans {output_dir}:\")\n",
        "for file in output_files:\n",
        "    file_path = os.path.join(output_dir, file)\n",
        "    file_size = os.path.getsize(file_path) / 1024  # Taille en Ko\n",
        "    print(f\"- {file} ({file_size:.2f} Ko)\")\n",
        "    \n",
        "    # Afficher les premières lignes de chaque fichier\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(f\"  Nombre de lignes: {len(df)}\")\n",
        "    display(df.head(3))\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Prochaine étape\n",
        "\n",
        "Maintenant que les données ont été transformées et enregistrées dans les fichiers CSV appropriés, vous pouvez passer à la phase de chargement en exécutant le notebook `Chargement_v2.ipynb` dans le dossier `Partie-03-Chargement`.\n",
        "\n",
        "Assurez-vous que votre base de données MySQL est configurée correctement avant d'exécuter ce notebook. Les paramètres de connexion sont :\n",
        "- Nom de la base de données: epiviz\n",
        "- Utilisateur: root\n",
        "- Mot de passe: (aucun)\n",
        "- Hôte: localhost\n",
        "- Port: 3306"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}